{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "from utilities import chordUtil\n",
    "from utilities import dataImport\n",
    "from utilities.chordUtil import *\n",
    "from utilities.dataImport import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"use_cuda\")\n",
    "print(use_cuda)\n",
    "#use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "{'E:dim': 0, 'E:maj': 1, 'A:maj': 2, 'F:dim': 3, 'B:maj': 4, 'B:min': 5, 'C:min': 6, 'D:min': 7, 'F#:maj': 8, 'D:dim': 9, 'A:dim': 10, 'A#:maj': 11, 'A#:min': 12, 'B:dim': 13, 'C#:dim': 14, 'E:min': 15, 'F:maj': 16, 'G:dim': 17, 'G#:min': 18, 'C#:min': 19, 'C:dim': 20, 'F:min': 21, 'D#:maj': 22, 'F#:min': 23, 'G:maj': 24, 'G#:dim': 25, 'F#:dim': 26, 'G:min': 27, 'D#:dim': 28, 'C:maj': 29, 'A#:dim': 30, 'C#:maj': 31, 'G#:maj': 32, 'D:maj': 33, 'D#:min': 34, 'A:min': 35, 'N': 36}\n"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "#lenSeq = 16\n",
    "lenSeq = 48\n",
    "alpha = 'a1'\n",
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "#filenames.remove(\".DS_Store\")\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alpha))\n",
    "print(len(dictChord))\n",
    "print(dictChord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "files_train ,files_test = train_test_split(filenames,test_size=0.7)\n",
    "dataset_train = dataImport.ChordSeqDataset(files_train, rootname, alpha, dictChord, lenSeq)\n",
    "dataset_test = dataImport.ChordSeqDataset(files_test, rootname, alpha, dictChord, lenSeq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generators\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "training_generator = data.DataLoader(dataset_train, **params)\n",
    "testing_generator = data.DataLoader(dataset_test, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(local_batch, local_labels):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    local_labels = torch.tensor([torch.argmax(local_label) for local_label in local_labels])\n",
    "    local_batch, local_labels = local_batch.to(device), local_labels.to(device)    \n",
    "    \n",
    "    output, (hidden, cell_state) = lstm_nn(local_batch)\n",
    "\n",
    "    output = output[:,-1,:]\n",
    "    loss = criterion(output, local_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    return output, loss.item() / len(local_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(local_batch, local_labels):\n",
    "    loss = 0\n",
    "\n",
    "    local_labels = torch.tensor([torch.argmax(local_label) for local_label in local_labels])\n",
    "    local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "    \n",
    "    output, (hidden, cell_state) = lstm_nn(local_batch)\n",
    "\n",
    "    output = output[:,-1,:]\n",
    "    loss = criterion(output, local_labels)\n",
    "\n",
    "    return output, loss.item() / len(local_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : 0m 9s (5 1%) -0.0417\n",
      "test : 0m 10s (5 1%) -0.0990\n",
      "train : 0m 19s (10 3%) -0.0473\n",
      "test : 0m 21s (10 3%) -0.0998\n",
      "train : 0m 30s (15 5%) -0.0468\n",
      "test : 0m 31s (15 5%) -0.1107\n",
      "train : 0m 41s (20 6%) -0.0470\n",
      "test : 0m 42s (20 6%) -0.1096\n",
      "train : 0m 51s (25 8%) -0.0475\n",
      "test : 0m 52s (25 8%) -0.1108\n",
      "train : 1m 2s (30 10%) -0.0448\n",
      "test : 1m 3s (30 10%) -0.1109\n",
      "train : 1m 12s (35 11%) -0.0474\n",
      "test : 1m 13s (35 11%) -0.1109\n",
      "train : 1m 23s (40 13%) -0.0436\n",
      "test : 1m 24s (40 13%) -0.1099\n",
      "train : 1m 33s (45 15%) -0.0475\n",
      "test : 1m 35s (45 15%) -0.1110\n",
      "train : 1m 44s (50 16%) -0.0476\n",
      "test : 1m 45s (50 16%) -0.1109\n",
      "train : 1m 54s (55 18%) -0.0475\n",
      "test : 1m 55s (55 18%) -0.1108\n",
      "train : 2m 5s (60 20%) -0.0475\n",
      "test : 2m 6s (60 20%) -0.1110\n",
      "train : 2m 16s (65 21%) -0.0475\n",
      "test : 2m 17s (65 21%) -0.1108\n",
      "train : 2m 26s (70 23%) -0.0475\n",
      "test : 2m 27s (70 23%) -0.1109\n",
      "train : 2m 37s (75 25%) -0.0476\n",
      "test : 2m 38s (75 25%) -0.1107\n",
      "train : 2m 47s (80 26%) -0.0475\n",
      "test : 2m 48s (80 26%) -0.1109\n",
      "train : 2m 58s (85 28%) -0.0475\n",
      "test : 2m 59s (85 28%) -0.1111\n",
      "train : 3m 8s (90 30%) -0.0476\n",
      "test : 3m 9s (90 30%) -0.1109\n",
      "train : 3m 19s (95 31%) -0.0476\n",
      "test : 3m 20s (95 31%) -0.1107\n"
     ]
    }
   ],
   "source": [
    "lstm_nn = nn.LSTM(len(dictChord), len(dictChord), num_layers = 15, batch_first = True, dropout = 0.1).to(device=device)\n",
    "\n",
    "all_losses = []\n",
    "test_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "test_loss = 0\n",
    "start = time.time()\n",
    "\n",
    "#optimizer = torch.optim.Adam(lstm_nn.parameters(), lr = 1e-4)\n",
    "optimizer = torch.optim.SGD(lstm_nn.parameters(), lr = 5e-2, momentum=0.9)\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "print_every = 5\n",
    "plot_every = 5\n",
    "max_epochs = 300\n",
    "\n",
    "\n",
    "# Begin training\n",
    "\n",
    "for epoch in range(1, max_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU (nope later...)\n",
    "        #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        output, loss = train(local_batch, local_labels)\n",
    "        #print(loss)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('train : %s (%d %d%%) %.4f' % (timeSince(start), epoch, epoch / max_epochs * 100, loss))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(total_loss / (plot_every ))\n",
    "        total_loss = 0\n",
    "        \n",
    "\n",
    "    # Testing\n",
    "    for local_batch, local_labels in testing_generator:\n",
    "        # Transfer to GPU (nope later...)\n",
    "        #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        output, loss = test(local_batch, local_labels)\n",
    "        test_loss +=loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('test : %s (%d %d%%) %.4f' % (timeSince(start), epoch, epoch / max_epochs * 100, loss))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        test_losses.append(test_loss / (plot_every ))\n",
    "        test_loss = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses, label=\"train\")\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "#plt.show()\n",
    "plt.plot(test_losses, label=\"test\")\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_sequence = [\"C:maj\",\"F:maj\",\"C:maj\",\"C:maj\",\"F:maj\",\"F:maj\",\"C:maj\",\"C:maj\",\"G:maj\",\"F:maj\",\"C:maj\",\"G:maj\",\"C:maj\",\"C:maj\",\"F:maj\",\"C:maj\"]\n",
    "\n",
    "test_sequence = [\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\n",
    "                \"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\n",
    "                \"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\n",
    "                \"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\n",
    "                \"G:maj\",\"G:maj\",\"G:maj\",\"G:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\"F:maj\",\n",
    "                \"C:maj\",\"C:maj\",\"C:maj\",\"C:maj\",\"G:maj\",\"G:maj\",\"G:maj\",\"G:maj\",]\n",
    "\n",
    "test_sequence_tensor = torch.zeros(1, len(test_sequence), len(dictChord)).to(device)\n",
    "for t in range(len(test_sequence)):\n",
    "    test_sequence_tensor[0, t, dictChord[test_sequence[t]]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_lenght = 120\n",
    "\n",
    "generated_sequence = [0 for i in range(generation_lenght)]\n",
    "generated_sequence[0:lenSeq] = test_sequence\n",
    "\n",
    "\n",
    "\n",
    "for t in range(generation_lenght-lenSeq):\n",
    "    if t == 0:\n",
    "        output, (hidden, cell_state) = lstm_nn(test_sequence_tensor)\n",
    "        generated_sequence[t+lenSeq] = listChord[torch.argmax(output[0,-1]).item()]\n",
    "        \n",
    "    else : \n",
    "        last_chords_output = torch.zeros(1, lenSeq, len(dictChord)).to(device)\n",
    "        for i in range(lenSeq):\n",
    "            last_chords_output[0, i, torch.argmax(output[0][-1]).item()] = 1\n",
    "        #last_chords_output = torch.tensor([[torch.argmax(output[0][i]).item() for i in range(lenSeq)]])\n",
    "        last_chords_output.to(device)\n",
    "\n",
    "        \n",
    "        output, (hidden, cell_state) = lstm_nn(last_chords_output, (hidden, cell_state))\n",
    "        generated_sequence[t+lenSeq] = listChord[torch.argmax(output[0][-1]).item()]\n",
    "\n",
    "        \n",
    "for i in range(generation_lenght):\n",
    "    if i%4 == 0:\n",
    "        print(generated_sequence[i:i+4])\n",
    "    if i == lenSeq-1 :\n",
    "        print(\"generated :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
