{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"Adam\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"rnn\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 256,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_generator': 512,\n",
    "        'num_layers_generator': 5,\n",
    "        'dropout_generator': 0.01,\n",
    "        'learning_rate_generator': 5e-2, \n",
    "        'hidden_size_discriminator':512,\n",
    "        'num_layers_discriminator':5,\n",
    "        'dropout_discriminator': 0.01,\n",
    "        'learning_rate_discriminator': 5e-2, \n",
    "        'epochs': 50,\n",
    "        'use_Paul_distance': True}\n",
    "\n",
    "paramsGen = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"Adam\",\n",
    "        'lossFunction': \"CrossEntropy\",\n",
    "        'model_type': \"rnn\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_generator': 128,\n",
    "        'num_layers_generator': 2,\n",
    "        'dropout_generator': 0.01,\n",
    "        'learning_rate_generator': 5e-2, \n",
    "        'epochs': 50}\n",
    "\n",
    "paramsDis = {\n",
    "        'input_size':25,\n",
    "        'hidden_size_discriminator':128,\n",
    "        'num_layers_discriminator':2,\n",
    "        'dropout_discriminator': 0.01}\n",
    "\n",
    "\n",
    "#from GruGanClass import MYGRU\n",
    "#from RnnGanClass import MYRNN\n",
    "#from LstmGanClass import MYLSTM\n",
    "import torch\n",
    "import os\n",
    "from utilities import chordUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities import dataImport\n",
    "\n",
    "\n",
    "\n",
    "dropout_generator = params['dropout_generator']\n",
    "model_type = params['model_type']\n",
    "num_layers_generator = params['num_layers_generator']\n",
    "hidden_size_generator = params['hidden_size_generator']\n",
    "alphabet = params['alphabet']\n",
    "sequence_lenght_in = params['sequence_lenght_in']\n",
    "sequence_lenght_out = params['sequence_lenght_out']\n",
    "num_layers_discriminator = params['num_layers_discriminator']\n",
    "hidden_size_discriminator = params['hidden_size_discriminator']\n",
    "\n",
    "#dropoutstr = str(dropout).replace('.',',')\n",
    "model_string_generator = \"models/\"+model_type+str(num_layers_generator)+\"layers\"+str(hidden_size_generator)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght_in)+\"lenSeq_in\"+str(sequence_lenght_out)+\"lenSeq_out.pt\"\n",
    "model_string_discriminator = \"models/\"+model_type+str(num_layers_discriminator)+\"layers\"+str(hidden_size_discriminator)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght_out)+\"lenSeq_out.pt\"\n",
    "\n",
    "# Getting alphabet size :\n",
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "alphabet_size = len(dictChord)\n",
    "#print(dictChord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "print(len(dictChord))\n",
    "\n",
    "# Create datasets\n",
    "#files_train ,files_test = train_test_split(filenames,test_size=0.7)\n",
    "#if training:\n",
    "#    dataset_train = dataImport.ChordSeqDataset(files_train, rootname, alphabet, dictChord, sequence_lenght)\n",
    "#dataset_test = dataImport.ChordSeqDataset(files_test, rootname, alphabet, dictChord, sequence_lenght)\n",
    "\n",
    "#dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_in+sequence_lenght_out)\n",
    "#params[\"dataset\"] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "from utilities import chordUtil\n",
    "from utilities import dataImport\n",
    "from utilities import plotAndTimeUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities.dataImport import *\n",
    "from utilities.plotAndTimeUtil import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from utilities import chordsDistances\n",
    "from utilities.chordsDistances import getPaulMatrix\n",
    "from utilities import remapChordsToBase\n",
    "from utilities.remapChordsToBase import remapPaulToTristan\n",
    "from random import choices\n",
    "\n",
    "\n",
    "class MYRNN(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_generator, num_layers_generator, dropout_generator):\n",
    "        super(MYRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size_generator, num_layers = num_layers_generator, batch_first = True, dropout = dropout_generator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_generator, input_size)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "    # Defining some cool functions to make work easier and trop styl√©\n",
    "\n",
    "    # About the trainingData :\n",
    "    # 0 is training number\n",
    "    # 1 is optimizer used\n",
    "    # 2 is loss used\n",
    "    # 3 is loss values on train set\n",
    "    # 4 is loss values on test set\n",
    "    # 5 is accuracy on train set\n",
    "    # 6 is accuracy on test set\n",
    "    # 7 is time\n",
    "    # 8 is learning rate\n",
    "    # 9 is use_Paul_distance\n",
    "\n",
    "        \n",
    "        # Usefull for monitoring\n",
    "        self.trainingData = [[0],[\"None\"],[\"None\"],[[0]],[[0]],[[0]],[[0]],[0],[0],[\"No\"]]\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        output = self.softmax(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        return output\n",
    "\n",
    "    def trainOnDis(self, model_type, print_every, optimizer, lossFunction, disNet, alphabet='a0', sequence_lenght_in=16, sequence_lenght_out=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size_generator=128, num_layers_generator=2, dropout_generator=0.1, learning_rate_generator=1e-4, epochs=10, sampling=True):\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, similarity = self.doEpochs(model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, disNet, sampling, training=True)\n",
    "   \n",
    "        print(\"Finished Training\")\n",
    "        return\n",
    "\n",
    "    def doEpochs(self, model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, disNet, sampling, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        total_loss = 0 # Reset every plot_every iters\n",
    "        genWin = 0\n",
    "        totalSize = 0\n",
    "        genWinTest = []\n",
    "\n",
    "        # Creating Dataset\n",
    "        dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_in+sequence_lenght_out)\n",
    "        \n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "            \n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset, **params)\n",
    "\n",
    "        #TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate_generator)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        #TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    output, loss, genWinOne = self.oneBatchTrain(local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, disNet)\n",
    "                    total_loss += loss\n",
    "                    genWin += genWinOne\n",
    "                    totalSize += len(local_batch)\n",
    "\n",
    "                if epoch % print_every == 0 and epoch != 0:\n",
    "                    genWin = genWin/totalSize\n",
    "                    total_loss = total_loss/totalSize\n",
    "                    genWinTest.append(genWin)\n",
    "                    all_losses.append(total_loss)\n",
    "                    print('genWin : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, total_loss, genWin*100))\n",
    "                    genWin = 0\n",
    "                    total_loss = 0\n",
    "                    \n",
    "            # Testing\n",
    "            #self.train(mode=False)\n",
    "\n",
    "        return all_losses, genWinTest\n",
    "\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrain(self, local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, disNet):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess = 0\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor([torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "        \n",
    "\n",
    "        local_batch = local_batch.to(device)\n",
    "        #local_labels = local_labels.to(device)\n",
    "        for i in range(sequence_lenght_out):\n",
    "            output = self.forward(local_batch)\n",
    "            local_batch[:,0:sequence_lenght_in-1,:] = local_batch[:,1:sequence_lenght_in,:]\n",
    "            \n",
    "            if sampling:\n",
    "                choice = choices(range(len(listChord)),softmax(output[0]))[0]\n",
    "                local_batch[:,sequence_lenght_in-1, choice] = 1\n",
    "            else:\n",
    "                #TODO\n",
    "                generated_sequence[sequence_lenght_in] = listChord[torch.argmax(output_probability).item()]\n",
    "                local_batch[:, sequence_lenght_in, torch.argmax(output[:,:]).item()] = 1                \n",
    "                \n",
    "        print(local_batch.size())\n",
    "        #local_batch has been transformed in output\n",
    "        disNet.to(device)\n",
    "        disDecision = disNet(local_batch)\n",
    "        \n",
    "        #TODO : change that 0 in something more relevant\n",
    "        correct_guess = 0 \n",
    "        \n",
    "        loss = criterion(disDecision, torch.ones([len(local_batch)], dtype=torch.int64).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return output, loss.item(), correct_guess\n",
    "\n",
    "\n",
    "\n",
    "    def plotLastTraining(self):\n",
    "        plotAndTimeUtil.PlotResults(self.trainingData[3][-1], self.trainingData[4][-1], self.trainingData[5][-1], self.trainingData[6][-1])\n",
    "\n",
    "    def plotAllTraining(self):\n",
    "        plotAndTimeUtil.PlotAllResults(self.trainingData)\n",
    "\n",
    "    def toString(self, model_type, print_every, plot_every, optimizer, lossFunction, alphabet='a0', sequence_lenght=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size=128, num_layers=2, dropout=0.1, learning_rate=1e-4, epochs=10, use_Paul_distance=False):\n",
    "        #dropoutstr = str(dropout).replace('.',',')\n",
    "        model_string = \"models/\"+model_type+str(num_layers)+\"layers\"+str(hidden_size)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght)+\"lenSeq.pt\"\n",
    "        return model_string\n",
    "    \n",
    "    def generateFromSequence(self, test_sequence, generation_lenght, alphabet, sampling=False, using_cuda=True, silent=True):\n",
    "        lenSeq = len(test_sequence)\n",
    "        \n",
    "        # Cuda blabla\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)    \n",
    "        \n",
    "        # Getting chords dictionary\n",
    "        rootname = \"inputs/jazz_xlab/\"\n",
    "        filenames = os.listdir(rootname)\n",
    "        dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "        \n",
    "        \n",
    "        # Initialising objects\n",
    "        test_sequence_tensor = torch.zeros(1, len(test_sequence), len(dictChord)).to(device)\n",
    "        last_chords_output = torch.zeros(1, lenSeq, len(dictChord)).to(device)\n",
    "        test_sequence_tensor.requires_grad = False\n",
    "        last_chords_output.requires_grad = False\n",
    "        for t in range(len(test_sequence)):\n",
    "            test_sequence_tensor[0, t, dictChord[test_sequence[t]]] = 1\n",
    "            if t != len(test_sequence)-1 :\n",
    "                last_chords_output[0, t-1, dictChord[test_sequence[t]]] = 1\n",
    "\n",
    "                \n",
    "        generated_sequence = [0 for i in range(generation_lenght)]\n",
    "        generated_sequence[0:lenSeq] = test_sequence\n",
    "\n",
    "        self.train(mode=False)\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        for t in range(generation_lenght-lenSeq):\n",
    "            if t == 0:\n",
    "                output_probability = self(test_sequence_tensor)\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else: \n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output_probability).item()]\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                last_chords_output.to(device)        \n",
    "                output_probability = self(last_chords_output)\n",
    "                last_chords_output[0,0:lenSeq-1] = last_chords_output[0,1:lenSeq]\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else:\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output).item()]\n",
    "\n",
    "\n",
    "        for i in range(generation_lenght):\n",
    "            if i%4 == 0:\n",
    "                print(generated_sequence[i:i+4])\n",
    "            if i == lenSeq-1 :\n",
    "                print(\"generated :\")\n",
    "                \n",
    "        if silent:\n",
    "            return\n",
    "        else:\n",
    "            return generated_sequence\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"gru\":\n",
    "    myNetwork = MYGRU(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)\n",
    "elif model_type == \"lstm\":\n",
    "    myNetwork = MYLSTM(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)\n",
    "elif model_type == \"rnn\":\n",
    "    myNetwork = MYRNN(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myNetwork = torch.load(model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDis(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_discriminator, num_layers_discriminator, dropout_discriminator):\n",
    "        super(MYDis, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size_discriminator, num_layers = num_layers_discriminator, batch_first = True, dropout = dropout_discriminator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_discriminator, 1)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Usefull for monitoring\n",
    "        #self.trainingData = [[0],[\"None\"],[\"None\"],[[0]],[[0]],[[0]],[[0]],[0],[0],[\"No\"]]\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        #output = self.softmax(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDis = MYDis(**paramsDis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-dfb8ed6b1725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparamsGen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'disNet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyDis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmyNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainOnDis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparamsGen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-ba4444bda61b>\u001b[0m in \u001b[0;36mtrainOnDis\u001b[0;34m(self, model_type, print_every, optimizer, lossFunction, disNet, alphabet, sequence_lenght_in, sequence_lenght_out, using_cuda, batch_size, shuffle, num_workers, hidden_size_generator, num_layers_generator, dropout_generator, learning_rate_generator, epochs, sampling)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using Cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorCopy.cpp:20"
     ]
    }
   ],
   "source": [
    "paramsGen['epochs']=4\n",
    "paramsGen['learning_rate_generator']=1e-4\n",
    "paramsGen['dropout_generator']=0.0\n",
    "paramsGen['disNet']=myDis\n",
    "\n",
    "myNetwork.trainOnDis(**paramsGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
