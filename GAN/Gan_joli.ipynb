{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"SGD\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"gru\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 256,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_generator': 512,\n",
    "        'num_layers_generator': 2,\n",
    "        'dropout_generator': 0.01,\n",
    "        'learning_rate_generator': 5e-2, \n",
    "        'hidden_size_discriminator':512,\n",
    "        'num_layers_discriminator':2,\n",
    "        'dropout_discriminator': 0.01,\n",
    "        'learning_rate_discriminator': 5e-2, \n",
    "        'epochs': 50,\n",
    "        'use_Paul_distance': True}\n",
    "\n",
    "paramsGen = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"SGD\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"gru\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_generator': 512,\n",
    "        'num_layers_generator': 2,\n",
    "        'dropout_generator': 0.01,\n",
    "        'learning_rate_generator': 1e-4, \n",
    "        'epochs': 50}\n",
    "\n",
    "paramsGenOnData = {\n",
    "        'print_every': 5,\n",
    "        'plot_every': 5,\n",
    "        'optimizer': \"SGD\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"gru\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size': 512,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.05,\n",
    "        'learning_rate': 5e-2, \n",
    "        'epochs': 50,\n",
    "        'use_Paul_distance': True}\n",
    "\n",
    "paramsDisInit = {\n",
    "        'input_size':25,\n",
    "        'hidden_size_discriminator':512,\n",
    "        'num_layers_discriminator':2,\n",
    "        'dropout_discriminator': 0.01}\n",
    "\n",
    "paramsDisOnGen = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"SGD\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"gru\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_discriminator': 512,\n",
    "        'num_layers_discriminator': 2,\n",
    "        'dropout_discriminator': 0.01,\n",
    "        'learning_rate_discriminator': 5e-2, \n",
    "        'epochs': 50\n",
    "}\n",
    "\n",
    "paramsDisOnData = {\n",
    "        'print_every': 5,\n",
    "        'optimizer': \"SGD\",\n",
    "        'lossFunction': \"MSE\",\n",
    "        'model_type': \"gru\",\n",
    "        'alphabet': 'a0',\n",
    "        'sequence_lenght_in': 16, \n",
    "        'sequence_lenght_out': 16, \n",
    "        'using_cuda': True,\n",
    "        'batch_size': 128,\n",
    "        'shuffle': True, \n",
    "        'num_workers': 6,\n",
    "        'hidden_size_discriminator': 256,\n",
    "        'num_layers_discriminator': 2,\n",
    "        'dropout_discriminator': 0.01,\n",
    "        'learning_rate_discriminator': 5e-2, \n",
    "        'epochs': 50\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "#from GruGanClass import MYGRU\n",
    "#from RnnGanClass import MYRNN\n",
    "#from LstmGanClass import MYLSTM\n",
    "import torch\n",
    "import os\n",
    "from utilities import chordUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities import dataImport\n",
    "\n",
    "\n",
    "\n",
    "dropout_generator = params['dropout_generator']\n",
    "model_type = params['model_type']\n",
    "num_layers_generator = params['num_layers_generator']\n",
    "hidden_size_generator = params['hidden_size_generator']\n",
    "alphabet = params['alphabet']\n",
    "sequence_lenght_in = params['sequence_lenght_in']\n",
    "sequence_lenght_out = params['sequence_lenght_out']\n",
    "num_layers_discriminator = params['num_layers_discriminator']\n",
    "hidden_size_discriminator = params['hidden_size_discriminator']\n",
    "\n",
    "#dropoutstr = str(dropout).replace('.',',')\n",
    "model_string_generator = \"models/\"+model_type+str(num_layers_generator)+\"layers\"+str(hidden_size_generator)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght_in)+\"lenSeq_in\"+str(sequence_lenght_out)+\"lenSeq_out.pt\"\n",
    "model_string_discriminator = \"models/\"+model_type+str(num_layers_discriminator)+\"layers\"+str(hidden_size_discriminator)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght_out)+\"lenSeq_out.pt\"\n",
    "\n",
    "# Getting alphabet size :\n",
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "alphabet_size = len(dictChord)\n",
    "#print(dictChord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "{'A:min': 0, 'C#:min': 1, 'A:maj': 2, 'G:min': 3, 'A#:min': 4, 'E:min': 5, 'B:maj': 6, 'G#:min': 7, 'F#:min': 8, 'F:min': 9, 'C:min': 10, 'B:min': 11, 'C:maj': 12, 'N': 13, 'C#:maj': 14, 'G#:maj': 15, 'F:maj': 16, 'D#:min': 17, 'E:maj': 18, 'D:min': 19, 'D:maj': 20, 'A#:maj': 21, 'F#:maj': 22, 'D#:maj': 23, 'G:maj': 24}\n"
     ]
    }
   ],
   "source": [
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "print(len(dictChord))\n",
    "print(dictChord)\n",
    "\n",
    "# Create datasets\n",
    "#files_train ,files_test = train_test_split(filenames,test_size=0.7)\n",
    "#if training:\n",
    "#    dataset_train = dataImport.ChordSeqDataset(files_train, rootname, alphabet, dictChord, sequence_lenght)\n",
    "#dataset_test = dataImport.ChordSeqDataset(files_test, rootname, alphabet, dictChord, sequence_lenght)\n",
    "\n",
    "#dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_in+sequence_lenght_out)\n",
    "#params[\"dataset\"] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "from utilities import chordUtil\n",
    "from utilities import dataImport\n",
    "from utilities import plotAndTimeUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities.dataImport import *\n",
    "from utilities.plotAndTimeUtil import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from utilities import chordsDistances\n",
    "from utilities.chordsDistances import getPaulMatrix\n",
    "from utilities import remapChordsToBase\n",
    "from utilities.remapChordsToBase import remapPaulToTristan\n",
    "from random import choices\n",
    "\n",
    "\n",
    "class MYGRU(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_generator, num_layers_generator, dropout_generator):\n",
    "        super(MYGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size_generator, num_layers = num_layers_generator, batch_first = True, dropout = dropout_generator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_generator, input_size)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "    # Defining some cool functions to make work easier and trop stylÃ©\n",
    "\n",
    "    # About the trainingData :\n",
    "    # 0 is training number\n",
    "    # 1 is optimizer used\n",
    "    # 2 is loss used\n",
    "    # 3 is loss values on train set\n",
    "    # 4 is loss values on test set\n",
    "    # 5 is accuracy on train set\n",
    "    # 6 is accuracy on test set\n",
    "    # 7 is time\n",
    "    # 8 is learning rate\n",
    "    # 9 is use_Paul_distance\n",
    "\n",
    "        \n",
    "        # Usefull for monitoring\n",
    "        self.trainingData = [[0],[\"None\"],[\"None\"],[[0]],[[0]],[[0]],[[0]],[0],[0],[\"No\"]]\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        output = self.softmax(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def trainAndTest(self, model_type, print_every, plot_every, optimizer, lossFunction, alphabet='a0', sequence_lenght=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size=128, num_layers=2, dropout=0.1, learning_rate=1e-4, epochs=10, use_Paul_distance=False):\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, test_losses, accuracy_train, accuracy_test = self.doEpochs(\n",
    "            model_type, epochs, print_every, plot_every, optimizer, lossFunction, learning_rate, batch_size, shuffle, num_workers, alphabet, sequence_lenght, device, start, use_Paul_distance, training=True)\n",
    "\n",
    "        self.trainingData[0].append(self.trainingData[0][-1]+1)\n",
    "        self.trainingData[1].append(optimizer)\n",
    "        self.trainingData[2].append(lossFunction)\n",
    "        self.trainingData[3].append(all_losses)\n",
    "        self.trainingData[4].append(test_losses)\n",
    "        self.trainingData[5].append(accuracy_train)\n",
    "        self.trainingData[6].append(accuracy_test)\n",
    "        self.trainingData[7].append(plotAndTimeUtil.timeSince(start))\n",
    "        self.trainingData[8].append(learning_rate)\n",
    "        self.trainingData[9].append(use_Paul_distance)\n",
    "\n",
    "        print(\"Finished Training\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def doEpochs(self, model_type, epochs, print_every, plot_every, optimizer, lossFunction, learning_rate, batch_size, shuffle, num_workers, alphabet, sequence_lenght, device, start, use_Paul_distance, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        test_losses = []\n",
    "        total_loss = 0  # Reset every plot_every iters\n",
    "        test_loss = 0\n",
    "        correct_guess_train, wrong_guess_train, correct_guess_test, wrong_guess_test = 0, 0, 0, 0\n",
    "        accuracy_test = []\n",
    "        accuracy_train = []\n",
    "\n",
    "        rootname = \"inputs/jazz_xlab/\"\n",
    "        filenames = os.listdir(rootname)\n",
    "        dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "\n",
    "        # Create datasets\n",
    "        files_train, files_test = train_test_split(filenames, test_size=0.7)\n",
    "        if training:\n",
    "            dataset_train = dataImport.ChordSeqDataset(\n",
    "                files_train, rootname, alphabet, dictChord, sequence_lenght)\n",
    "        dataset_test = dataImport.ChordSeqDataset(\n",
    "            files_test, rootname, alphabet, dictChord, sequence_lenght)\n",
    "\n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "\n",
    "        # Get Paul distance matrix :\n",
    "        if use_Paul_distance:\n",
    "            M = chordsDistances.getPaulMatrix()\n",
    "            M = remapChordsToBase.remapPaulToTristan(M, alphabet)\n",
    "            distance_tensor = torch.tensor(M, dtype=torch.float).to(device)\n",
    "        else:\n",
    "            distance_tensor = 0\n",
    "\n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset_train, **params)\n",
    "        testing_generator = data.DataLoader(dataset_test, **params)\n",
    "\n",
    "        # TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        if optimizer == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        # TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        if lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "        for epoch in range(1, epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    output, loss, correct_guess, wrong_guess = self.oneBatchTrain(\n",
    "                        local_batch, local_labels, optimizer, criterion, device, use_Paul_distance, distance_tensor)\n",
    "                    total_loss += loss\n",
    "                    correct_guess_train += correct_guess\n",
    "                    wrong_guess_train += wrong_guess\n",
    "\n",
    "                if epoch % print_every == 0:\n",
    "                    accuracy = correct_guess_train / \\\n",
    "                        (correct_guess_train+wrong_guess_train)\n",
    "                    print('Train : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (\n",
    "                        plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, loss, accuracy*100))\n",
    "\n",
    "                if epoch % plot_every == 0:\n",
    "                    accuracy = correct_guess_train / \\\n",
    "                        (correct_guess_train+wrong_guess_train)\n",
    "                    all_losses.append(total_loss / (plot_every))\n",
    "                    accuracy_train.append(accuracy*100)\n",
    "                    total_loss = 0\n",
    "                    correct_guess_train, wrong_guess_train = 0, 0\n",
    "\n",
    "            # Testing\n",
    "            self.train(mode=False)\n",
    "            for local_batch, local_labels in testing_generator:\n",
    "                output, loss, correct_guess, wrong_guess = self.oneBatchTest(\n",
    "                    local_batch, local_labels, optimizer, criterion, device, use_Paul_distance, distance_tensor)\n",
    "                test_loss += loss\n",
    "\n",
    "                correct_guess_test += correct_guess\n",
    "                wrong_guess_test += wrong_guess\n",
    "\n",
    "            if epoch % print_every == 0:\n",
    "                accuracy = correct_guess_test / \\\n",
    "                    (correct_guess_test+wrong_guess_test)\n",
    "                if training:\n",
    "                    print('Test : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (\n",
    "                        plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, loss, accuracy*100))\n",
    "\n",
    "            if epoch % plot_every == 0:\n",
    "                accuracy = correct_guess_test / \\\n",
    "                    (correct_guess_test+wrong_guess_test)\n",
    "                test_losses.append(test_loss / (plot_every))\n",
    "                accuracy_test.append(accuracy*100)\n",
    "                test_loss = 0\n",
    "                correct_guess_test, wrong_guess_test = 0, 0\n",
    "\n",
    "        if training:\n",
    "            return all_losses, test_losses, accuracy_train, accuracy_test\n",
    "        else:\n",
    "            return test_losses, accuracy_test\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrain(self, local_batch, local_labels, optimizer, criterion, device, use_Paul_distance, distance_tensor):\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess, wrong_guess = 0, 0\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor(\n",
    "        #    [torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "\n",
    "        local_batch, local_labels = local_batch.to(\n",
    "            device), local_labels.to(device)\n",
    "        output = self.forward(local_batch)\n",
    "\n",
    "        for i in range(len(local_batch)):\n",
    "            if torch.argmax(output[i]) == torch.argmax(local_labels[i]):\n",
    "                correct_guess += 1\n",
    "            else:\n",
    "                wrong_guess += 1\n",
    "\n",
    "        # using Paul Distance\n",
    "        if use_Paul_distance:\n",
    "            loss_mult_coeff = 0\n",
    "            for i in range(len(local_batch)):\n",
    "                loss_mult_coeff += torch.matmul(distance_tensor, local_labels[i])[\n",
    "                    torch.argmax(output[i])]\n",
    "\n",
    "            loss_mult_coeff = loss_mult_coeff/len(local_batch)\n",
    "        else:\n",
    "            loss_mult_coeff = 1\n",
    "\n",
    "        loss = loss_mult_coeff * criterion(output, local_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return output, loss.item() / len(local_batch), correct_guess, wrong_guess\n",
    "\n",
    "\n",
    "# Defining test on one batch function\n",
    "\n",
    "    def oneBatchTest(self, local_batch, local_labels, optimizer, criterion, device, use_Paul_distance, distance_tensor):\n",
    "        loss = 0\n",
    "        correct_guess, wrong_guess = 0, 0\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor(\n",
    "        #    [torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "        local_batch, local_labels = local_batch.to(\n",
    "            device), local_labels.to(device)\n",
    "        output = self.forward(local_batch)\n",
    "\n",
    "        for i in range(len(local_batch)):\n",
    "            if torch.argmax(output[i]) == torch.argmax(local_labels[i]):\n",
    "                correct_guess += 1\n",
    "            else:\n",
    "                wrong_guess += 1\n",
    "\n",
    "        # using Paul Distance\n",
    "        if use_Paul_distance:\n",
    "            loss_mult_coeff = 0\n",
    "            for i in range(len(local_batch)):\n",
    "                loss_mult_coeff += torch.matmul(distance_tensor, local_labels[i])[\n",
    "                    torch.argmax(output[i])]\n",
    "\n",
    "            loss_mult_coeff = loss_mult_coeff/len(local_batch)\n",
    "        else:\n",
    "            loss_mult_coeff = 1\n",
    "\n",
    "        loss = criterion(output, local_labels)\n",
    "\n",
    "        return output, loss.item() / len(local_batch), correct_guess, wrong_guess\n",
    "\n",
    "\n",
    "    def trainOnDis(self, model_type, print_every, optimizer, lossFunction, disNet, alphabet='a0', sequence_lenght_in=16, sequence_lenght_out=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size_generator=128, num_layers_generator=2, dropout_generator=0.1, learning_rate_generator=1e-4, epochs=10, sampling=True):\n",
    "        #disNet.train(mode=False)\n",
    "        self.train(mode=True)\n",
    "        \n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, similarity = self.doEpochsOnDis(model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, disNet, sampling, training=True)\n",
    "   \n",
    "        print(\"Finished Training\")\n",
    "        return similarity\n",
    "\n",
    "    def doEpochsOnDis(self, model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, disNet, sampling, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        total_loss = 0 # Reset every plot_every iters\n",
    "        genWin = 0\n",
    "        totalSize = 0\n",
    "        genWinTest = []\n",
    "\n",
    "        # Creating Dataset\n",
    "        dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_in+sequence_lenght_out)\n",
    "        \n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "            \n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset, **params)\n",
    "\n",
    "        #TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate_generator)\n",
    "        elif optimizer ==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr = learning_rate_generator)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        #TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    output, loss, genWinOne = self.oneBatchTrainOnDis(local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, disNet)\n",
    "                    total_loss += loss\n",
    "                    genWin += genWinOne\n",
    "                    totalSize += len(local_batch)\n",
    "\n",
    "                if epoch % print_every == 0 and epoch != 0:\n",
    "                    genWin = genWin/totalSize\n",
    "                    total_loss = total_loss/totalSize\n",
    "                    genWinTest.append(genWin)\n",
    "                    all_losses.append(total_loss)\n",
    "                    print('genWin : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, 100*total_loss, genWin*100))\n",
    "                    genWin = 0\n",
    "                    total_loss = 0\n",
    "                    totalSize = 0\n",
    "                    \n",
    "            # Testing\n",
    "            #self.train(mode=False)\n",
    "\n",
    "        return all_losses, genWinTest\n",
    "\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrainOnDis(self, local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, disNet):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess = 0\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        temperature = 2\n",
    "\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor([torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "        \n",
    "\n",
    "        #local_labels = local_labels.to(device)\n",
    "        working_batch = torch.zeros([len(local_batch),sequence_lenght_in+sequence_lenght_out,len(local_batch[0,0])])\n",
    "        working_batch[:,0:sequence_lenght_in,:] = local_batch[:,0:sequence_lenght_in,:]\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        for i in range(sequence_lenght_out):\n",
    "            output = self.forward(local_batch)\n",
    "            #local_batch[:,0:sequence_lenght_in-1,:] = local_batch[:,1:sequence_lenght_in,:]\n",
    "            \n",
    "            if sampling:\n",
    "                #choice = choices(range(len(listChord)),softmax(output[0]))[0]\n",
    "                #print(output.size())\n",
    "                output = softmax(output)\n",
    "                _output = output.cpu().div(temperature).exp().data\n",
    "\n",
    "                #print(output.size())\n",
    "                topi = torch.multinomial(_output, 1)\n",
    "                #print(topi.size())\n",
    "                #print(working_batch.size())\n",
    "                for k in range(len(local_batch)):\n",
    "                    working_batch[k,sequence_lenght_in+i, topi[k,0].item()] = 1\n",
    "                #print(working_batch[0])\n",
    "            else:\n",
    "                #TODO\n",
    "                generated_sequence[sequence_lenght_in] = listChord[torch.argmax(output_probability).item()]\n",
    "                local_batch[:, sequence_lenght_in, torch.argmax(output[:,:]).item()] = 1                \n",
    "                \n",
    "        #print(working_batch.size())\n",
    "        #local_batch has been transformed in output\n",
    "        disNet.to(device)\n",
    "        disDecision = disNet(working_batch[:,sequence_lenght_in:,:].to(device))\n",
    "        \n",
    "        #TODO : change that 0 in something more relevant\n",
    "        \n",
    "        fooling = 0\n",
    "        for i in range(len(local_batch)):\n",
    "            if disDecision[i,0].item() > 0.5:\n",
    "                fooling+=1\n",
    "        #print(disDecision.size())\n",
    "        loss = criterion(disDecision, torch.ones([len(local_batch),1], dtype=torch.float).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return output, loss.item(), fooling\n",
    "\n",
    "    def plotLastTraining(self):\n",
    "        plotAndTimeUtil.PlotResults(self.trainingData[3][-1], self.trainingData[4][-1], self.trainingData[5][-1], self.trainingData[6][-1])\n",
    "\n",
    "    def plotAllTraining(self):\n",
    "        plotAndTimeUtil.PlotAllResults(self.trainingData)\n",
    "\n",
    "    def toString(self, model_type, print_every, plot_every, optimizer, lossFunction, alphabet='a0', sequence_lenght=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size=128, num_layers=2, dropout=0.1, learning_rate=1e-4, epochs=10, use_Paul_distance=False):\n",
    "        #dropoutstr = str(dropout).replace('.',',')\n",
    "        model_string = \"models/\"+model_type+str(num_layers)+\"layers\"+str(hidden_size)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght)+\"lenSeq.pt\"\n",
    "        return model_string\n",
    "    \n",
    "    def generateFromSequence(self, test_sequence, generation_lenght, alphabet, sampling=False, using_cuda=True, silent=True):\n",
    "        lenSeq = len(test_sequence)\n",
    "        \n",
    "        # Cuda blabla\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)    \n",
    "        \n",
    "        # Getting chords dictionary\n",
    "        rootname = \"inputs/jazz_xlab/\"\n",
    "        filenames = os.listdir(rootname)\n",
    "        dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "        \n",
    "        \n",
    "        # Initialising objects\n",
    "        test_sequence_tensor = torch.zeros(1, len(test_sequence), len(dictChord)).to(device)\n",
    "        last_chords_output = torch.zeros(1, lenSeq, len(dictChord)).to(device)\n",
    "        test_sequence_tensor.requires_grad = False\n",
    "        last_chords_output.requires_grad = False\n",
    "        for t in range(len(test_sequence)):\n",
    "            test_sequence_tensor[0, t, dictChord[test_sequence[t]]] = 1\n",
    "            if t != len(test_sequence)-1 :\n",
    "                last_chords_output[0, t-1, dictChord[test_sequence[t]]] = 1\n",
    "\n",
    "                \n",
    "        generated_sequence = [0 for i in range(generation_lenght)]\n",
    "        generated_sequence[0:lenSeq] = test_sequence\n",
    "\n",
    "        self.train(mode=False)\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        for t in range(generation_lenght-lenSeq):\n",
    "            if t == 0:\n",
    "                output_probability = self(test_sequence_tensor)\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else: \n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output_probability).item()]\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                last_chords_output.to(device)        \n",
    "                output_probability = self(last_chords_output)\n",
    "                last_chords_output[0,0:lenSeq-1] = last_chords_output[0,1:lenSeq]\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else:\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output).item()]\n",
    "\n",
    "\n",
    "        for i in range(generation_lenght):\n",
    "            if i%4 == 0:\n",
    "                print(generated_sequence[i:i+4])\n",
    "            if i == lenSeq-1 :\n",
    "                print(\"generated :\")\n",
    "                \n",
    "        if silent:\n",
    "            return\n",
    "        else:\n",
    "            return generated_sequence\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"gru\":\n",
    "    myGenerator = MYGRU(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)\n",
    "elif model_type == \"lstm\":\n",
    "    myGenerator = MYLSTM(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)\n",
    "elif model_type == \"rnn\":\n",
    "    myGenerator = MYRNN(alphabet_size, hidden_size_generator, num_layers_generator, dropout_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myNetwork = torch.load(model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDis(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_discriminator, num_layers_discriminator, dropout_discriminator):\n",
    "        super(MYDis, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size_discriminator, num_layers = num_layers_discriminator, batch_first = True, dropout = dropout_discriminator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_discriminator, 1)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "        # Usefull for monitoring\n",
    "        #self.trainingData = [[0],[\"None\"],[\"None\"],[[0]],[[0]],[[0]],[[0]],[0],[0],[\"No\"]]\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        #output = self.softmax(output)\n",
    "        #print(\"outputsize : \")\n",
    "        #print(output.size())\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def trainOnGen(self, model_type, print_every, optimizer, lossFunction, genNet, alphabet='a0', sequence_lenght_in=16, sequence_lenght_out=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size_discriminator=128, num_layers_discriminator=2, dropout_discriminator=0.1, learning_rate_discriminator=1e-4, epochs=10, sampling=True):\n",
    "        self.train(mode=True)\n",
    "        genNet.train(mode=False)\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, similarity = self.doEpochsOnGen(model_type, epochs, print_every, optimizer, lossFunction, learning_rate_discriminator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, genNet, sampling, training=True)\n",
    "   \n",
    "        print(\"Finished Training\")\n",
    "        return similarity\n",
    "\n",
    "    def doEpochsOnGen(self, model_type, epochs, print_every, optimizer, lossFunction, learning_rate_discriminator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, genNet, sampling, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        total_loss = 0 # Reset every plot_every iters\n",
    "        disOnGenWin = 0\n",
    "        totalSize = 0\n",
    "        disOnGenWinTest = []\n",
    "\n",
    "        # Creating Dataset\n",
    "        dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_in+sequence_lenght_out)\n",
    "        \n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "            \n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset, **params)\n",
    "\n",
    "        #TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate_discriminator)\n",
    "        elif optimizer ==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr = learning_rate_discriminator)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        #TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    output, loss, disOnGenWinOne = self.oneBatchTrainOnGen(local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, genNet)\n",
    "                    total_loss += loss\n",
    "                    disOnGenWin += disOnGenWinOne\n",
    "                    totalSize += len(local_batch)\n",
    "\n",
    "                if epoch % print_every == 0 and epoch != 0:\n",
    "                    disOnGenWin = disOnGenWin/totalSize\n",
    "                    total_loss = total_loss/totalSize\n",
    "                    disOnGenWinTest.append(disOnGenWin)\n",
    "                    all_losses.append(total_loss)\n",
    "                    print('disOnGen : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, total_loss*100, disOnGenWin*100))\n",
    "                    disOnGenWin = 0\n",
    "                    total_loss = 0\n",
    "                    totalSize = 0\n",
    "\n",
    "                    \n",
    "            # Testing\n",
    "            #self.train(mode=False)\n",
    "\n",
    "        return all_losses, disOnGenWinTest\n",
    "\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrainOnGen(self, local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling, genNet):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess = 0\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        temperature = 2\n",
    "\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor([torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "        \n",
    "\n",
    "        #local_labels = local_labels.to(device)\n",
    "        working_batch = torch.zeros([len(local_batch),sequence_lenght_in+sequence_lenght_out,len(local_batch[0,0])])\n",
    "        working_batch[:,0:sequence_lenght_in,:] = local_batch[:,0:sequence_lenght_in,:]\n",
    "        local_batch = local_batch.to(device)\n",
    "\n",
    "        for i in range(sequence_lenght_out):\n",
    "            output = genNet.forward(local_batch)\n",
    "            #local_batch[:,0:sequence_lenght_in-1,:] = local_batch[:,1:sequence_lenght_in,:]\n",
    "            \n",
    "            if sampling:\n",
    "                #choice = choices(range(len(listChord)),softmax(output[0]))[0]\n",
    "                #print(output.size())\n",
    "                output = softmax(output)\n",
    "                _output = output.cpu().div(temperature).exp().data\n",
    "\n",
    "                #print(output.size())\n",
    "                topi = torch.multinomial(_output, 1)\n",
    "                #print(topi.size())\n",
    "                #print(working_batch.size())\n",
    "                for k in range(len(local_batch)):\n",
    "                    working_batch[k,sequence_lenght_in+i, topi[k,0].item()] = 1\n",
    "                #print(working_batch[0])\n",
    "            else:\n",
    "                #TODO\n",
    "                generated_sequence[sequence_lenght_in] = listChord[torch.argmax(output_probability).item()]\n",
    "                local_batch[:, sequence_lenght_in, torch.argmax(output[:,:]).item()] = 1                \n",
    "                \n",
    "        #print(working_batch.size())\n",
    "        #local_batch has been transformed in output\n",
    "        self.to(device)\n",
    "        disDecision = self(working_batch[:,sequence_lenght_in:,:].to(device))\n",
    "        \n",
    "        #TODO : change that 0 in something more relevant\n",
    "        fooling = 0\n",
    "        for i in range(len(local_batch)):\n",
    "            if disDecision[i,0].item() < 0.5:\n",
    "                fooling+=1\n",
    "        #print(disDecision.size())\n",
    "        loss = criterion(disDecision, torch.zeros([len(local_batch),1], dtype=torch.float).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return output, loss.item(), fooling\n",
    "\n",
    "    def trainOnData(self, model_type, print_every, optimizer, lossFunction, alphabet='a0', sequence_lenght_in=16, sequence_lenght_out=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size_discriminator=128, num_layers_discriminator=2, dropout_discriminator=0.1, learning_rate_discriminator=1e-4, epochs=10, sampling=True):\n",
    "        self.train(mode=True)\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, similarity = self.doEpochsOnData(model_type, epochs, print_every, optimizer, lossFunction, learning_rate_discriminator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, sampling, training=True)\n",
    "   \n",
    "        print(\"Finished Training\")\n",
    "        return similarity\n",
    "\n",
    "    def doEpochsOnData(self, model_type, epochs, print_every, optimizer, lossFunction, learning_rate_discriminator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, sampling, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        total_loss = 0 # Reset every plot_every iters\n",
    "        disOnDataWin = 0\n",
    "        totalSize = 0\n",
    "        disOnDataWinTest = []\n",
    "\n",
    "        # Creating Dataset\n",
    "        dataset = dataImport.ChordSeqDataset(filenames, rootname, alphabet, dictChord, sequence_lenght_out)\n",
    "        \n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "            \n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset, **params)\n",
    "\n",
    "        #TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate_discriminator)\n",
    "        elif optimizer ==\"SGD\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr = learning_rate_discriminator)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        #TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    loss, disOnDataWinOne = self.oneBatchTrainOnData(local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling)\n",
    "                    total_loss += loss\n",
    "                    disOnDataWin += disOnDataWinOne\n",
    "                    totalSize += len(local_batch)\n",
    "\n",
    "                if epoch % print_every == 0 and epoch != 0:\n",
    "                    disOnDataWin = disOnDataWin/totalSize\n",
    "                    total_loss = total_loss/totalSize\n",
    "                    disOnDataWinTest.append(disOnDataWin)\n",
    "                    all_losses.append(total_loss)\n",
    "                    print('disOnDataWin : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, total_loss*100, disOnDataWin*100))\n",
    "                    disOnDataWin = 0\n",
    "                    total_loss = 0\n",
    "                    totalSize = 0\n",
    "\n",
    "                    \n",
    "            # Testing\n",
    "            #self.train(mode=False)\n",
    "\n",
    "        return all_losses, disOnDataWinTest\n",
    "\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrainOnData(self, local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess = 0\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        temperature = 2\n",
    "\n",
    "\n",
    "        self.to(device)\n",
    "        disDecision = self(local_batch.to(device))\n",
    "        \n",
    "        #TODO : change that 0 in something more relevant\n",
    "        fooling = 0\n",
    "        for i in range(len(local_batch)):\n",
    "            if disDecision[i,0].item() > 0.5:\n",
    "                fooling+=1\n",
    "        #print(disDecision.size())\n",
    "        loss = criterion(disDecision, torch.ones([len(local_batch),1], dtype=torch.float).to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return loss.item(), fooling\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDis = MYDis(**paramsDisInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "Train : 0m 26s (20 4%) loss : 0.3024, accuracy : 44.6893%\n",
      "Test : 0m 27s (20 4%) loss : 0.1460, accuracy : 45.7702%\n",
      "Train : 0m 54s (40 8%) loss : 0.2699, accuracy : 45.8851%\n",
      "Test : 0m 55s (40 8%) loss : 0.1460, accuracy : 47.3457%\n",
      "Train : 1m 22s (60 12%) loss : 0.2773, accuracy : 46.5651%\n",
      "Test : 1m 23s (60 12%) loss : 0.1460, accuracy : 46.7737%\n",
      "Train : 1m 50s (80 16%) loss : 0.2419, accuracy : 46.8230%\n",
      "Test : 1m 51s (80 16%) loss : 0.1460, accuracy : 46.6433%\n",
      "Train : 2m 18s (100 20%) loss : 0.2788, accuracy : 45.0176%\n",
      "Test : 2m 18s (100 20%) loss : 0.1460, accuracy : 46.8339%\n",
      "Train : 2m 46s (120 24%) loss : 0.2463, accuracy : 46.0727%\n",
      "Test : 2m 47s (120 24%) loss : 0.1460, accuracy : 47.5163%\n",
      "Train : 3m 19s (140 28%) loss : 0.2728, accuracy : 46.5885%\n",
      "Test : 3m 20s (140 28%) loss : 0.1460, accuracy : 47.3256%\n",
      "Train : 3m 48s (160 32%) loss : 0.3024, accuracy : 46.8933%\n",
      "Test : 3m 49s (160 32%) loss : 0.1460, accuracy : 46.8941%\n",
      "Train : 4m 17s (180 36%) loss : 0.3083, accuracy : 46.3072%\n",
      "Test : 4m 18s (180 36%) loss : 0.1460, accuracy : 48.0181%\n",
      "Train : 4m 48s (200 40%) loss : 0.2094, accuracy : 46.1430%\n",
      "Test : 4m 49s (200 40%) loss : 0.1460, accuracy : 47.7572%\n",
      "Train : 5m 18s (220 44%) loss : 0.2847, accuracy : 46.7995%\n",
      "Test : 5m 18s (220 44%) loss : 0.1460, accuracy : 47.6367%\n",
      "Train : 5m 47s (240 48%) loss : 0.3274, accuracy : 45.6506%\n",
      "Test : 5m 48s (240 48%) loss : 0.1460, accuracy : 46.2418%\n",
      "Train : 6m 16s (260 52%) loss : 0.2212, accuracy : 45.6975%\n",
      "Test : 6m 17s (260 52%) loss : 0.1460, accuracy : 47.6066%\n",
      "Train : 6m 45s (280 56%) loss : 0.2478, accuracy : 45.6506%\n",
      "Test : 6m 45s (280 56%) loss : 0.1460, accuracy : 47.9378%\n",
      "Train : 7m 13s (300 60%) loss : 0.2419, accuracy : 48.7691%\n",
      "Test : 7m 14s (300 60%) loss : 0.1460, accuracy : 47.4059%\n",
      "Train : 7m 41s (320 64%) loss : 0.2965, accuracy : 48.5111%\n",
      "Test : 7m 42s (320 64%) loss : 0.1460, accuracy : 48.0181%\n",
      "Train : 8m 10s (340 68%) loss : 0.2773, accuracy : 47.1747%\n",
      "Test : 8m 10s (340 68%) loss : 0.1460, accuracy : 48.8610%\n",
      "Train : 8m 38s (360 72%) loss : 0.3097, accuracy : 46.2603%\n",
      "Test : 8m 39s (360 72%) loss : 0.1460, accuracy : 47.4762%\n",
      "Train : 9m 6s (380 76%) loss : 0.2552, accuracy : 46.9637%\n",
      "Test : 9m 7s (380 76%) loss : 0.1460, accuracy : 48.3994%\n",
      "Train : 9m 34s (400 80%) loss : 0.3481, accuracy : 45.9086%\n",
      "Test : 9m 35s (400 80%) loss : 0.1460, accuracy : 47.5665%\n",
      "Train : 10m 2s (420 84%) loss : 0.3156, accuracy : 46.0258%\n",
      "Test : 10m 3s (420 84%) loss : 0.1460, accuracy : 47.7672%\n",
      "Train : 10m 30s (440 88%) loss : 0.2581, accuracy : 47.7140%\n",
      "Test : 10m 31s (440 88%) loss : 0.1460, accuracy : 48.1184%\n",
      "Train : 10m 58s (460 92%) loss : 0.3496, accuracy : 47.7843%\n",
      "Test : 10m 59s (460 92%) loss : 0.1460, accuracy : 47.9277%\n",
      "Train : 11m 26s (480 96%) loss : 0.2625, accuracy : 47.1747%\n",
      "Test : 11m 27s (480 96%) loss : 0.1460, accuracy : 48.1686%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "paramsGenOnData['epochs']=500\n",
    "paramsGenOnData['print_every']=20\n",
    "myGenerator.trainAndTest(**paramsGenOnData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 43s (5 10%) loss : 0.2031, accuracy : 39.8513%\n",
      "genWin : 1m 18s (10 20%) loss : 0.2031, accuracy : 39.6135%\n",
      "genWin : 1m 53s (15 30%) loss : 0.2031, accuracy : 40.1757%\n",
      "genWin : 2m 28s (20 40%) loss : 0.2032, accuracy : 39.8173%\n",
      "genWin : 3m 3s (25 50%) loss : 0.2031, accuracy : 39.5924%\n",
      "genWin : 3m 39s (30 60%) loss : 0.2031, accuracy : 40.2600%\n",
      "genWin : 4m 14s (35 70%) loss : 0.2031, accuracy : 40.2881%\n",
      "genWin : 4m 49s (40 80%) loss : 0.2030, accuracy : 40.6184%\n",
      "genWin : 5m 24s (45 90%) loss : 0.2031, accuracy : 39.9859%\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3985125322089482,\n",
       " 0.3961349262122277,\n",
       " 0.4017568517217147,\n",
       " 0.3981728742094167,\n",
       " 0.39592410400562195,\n",
       " 0.40260014054813775,\n",
       " 0.4028812368236121,\n",
       " 0.4061841180604357,\n",
       " 0.39985945186226285]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsGen['epochs']=50\n",
    "paramsGen['print_every']=5\n",
    "paramsGen['learning_rate_generator']=1e-0\n",
    "paramsGen['dropout_generator']=0.1\n",
    "paramsGen['disNet']=myDis\n",
    "\n",
    "myGenerator.trainOnDis(**paramsGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 20%) loss : 0.1982, accuracy : 79.9368%\n",
      "disOnGen : 0m 19s (2 40%) loss : 0.1941, accuracy : 98.1026%\n",
      "disOnGen : 0m 25s (3 60%) loss : 0.1915, accuracy : 99.7540%\n",
      "disOnGen : 0m 32s (4 80%) loss : 0.1888, accuracy : 99.9649%\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7993675333801827, 0.9810260014054814, 0.9975404075895994, 0.999648629655657]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsDisOnGen['epochs']=5\n",
    "paramsDisOnGen['print_every']=1\n",
    "paramsDisOnGen['learning_rate_discriminator']=5e-5\n",
    "paramsDisOnGen['dropout_discriminator']=0.1\n",
    "paramsDisOnGen['genNet']=myGenerator\n",
    "\n",
    "myDis.trainOnGen(**paramsDisOnGen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 20%) loss : 0.2162, accuracy : 1.8271%\n",
      "disOnDataWin : 0m 2s (2 40%) loss : 0.2145, accuracy : 4.4273%\n",
      "disOnDataWin : 0m 3s (3 60%) loss : 0.2135, accuracy : 6.3598%\n",
      "disOnDataWin : 0m 4s (4 80%) loss : 0.2121, accuracy : 8.3626%\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.018271257905832748,\n",
       " 0.04427266338721012,\n",
       " 0.06359803232607168,\n",
       " 0.08362614195361912]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramsDisOnData['epochs']=5\n",
    "paramsDisOnData['print_every']=1\n",
    "paramsDisOnData['learning_rate_discriminator']=2e-5\n",
    "paramsDisOnData['dropout_discriminator']=0.1\n",
    "\n",
    "myDis.trainOnData(**paramsDisOnData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration: 0\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.2013, accuracy : 56.1665%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2023, accuracy : 50.8609%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2004, accuracy : 59.4870%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.1998, accuracy : 62.1574%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1986, accuracy : 64.9684%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1971, accuracy : 70.9768%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1959, accuracy : 76.1068%\n",
      "Finished Training\n",
      "\n",
      "iteration: 1\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2034, accuracy : 37.7723%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1992, accuracy : 74.0337%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1964, accuracy : 90.0562%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2057, accuracy : 37.6669%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2040, accuracy : 44.3078%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2026, accuracy : 49.9297%\n",
      "Finished Training\n",
      "\n",
      "iteration: 2\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1969, accuracy : 86.4020%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2066, accuracy : 32.7477%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2048, accuracy : 42.3401%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2037, accuracy : 45.8538%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2028, accuracy : 49.3675%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2013, accuracy : 54.6381%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.2006, accuracy : 58.7140%\n",
      "Finished Training\n",
      "\n",
      "iteration: 3\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1991, accuracy : 73.0850%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2045, accuracy : 43.0604%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2025, accuracy : 51.0541%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2012, accuracy : 56.4652%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2005, accuracy : 59.1005%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1992, accuracy : 63.0358%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1979, accuracy : 68.1307%\n",
      "Finished Training\n",
      "\n",
      "iteration: 4\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2015, accuracy : 55.0246%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2020, accuracy : 52.4772%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2005, accuracy : 58.5734%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.1993, accuracy : 63.2115%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1982, accuracy : 67.7793%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1969, accuracy : 71.2228%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1955, accuracy : 75.8609%\n",
      "Finished Training\n",
      "\n",
      "iteration: 5\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2036, accuracy : 36.5249%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1995, accuracy : 71.9255%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1967, accuracy : 89.1075%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2054, accuracy : 38.6507%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2038, accuracy : 45.6781%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2026, accuracy : 48.9108%\n",
      "Finished Training\n",
      "\n",
      "iteration: 6\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1970, accuracy : 86.8587%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2065, accuracy : 32.3261%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2045, accuracy : 41.9536%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2036, accuracy : 45.4673%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2025, accuracy : 50.5622%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2014, accuracy : 55.5165%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.2002, accuracy : 58.6788%\n",
      "Finished Training\n",
      "\n",
      "iteration: 7\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1993, accuracy : 72.8918%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2041, accuracy : 43.0780%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2022, accuracy : 52.0379%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2012, accuracy : 55.6571%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2002, accuracy : 59.2762%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1987, accuracy : 65.1089%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1979, accuracy : 67.3928%\n",
      "Finished Training\n",
      "\n",
      "iteration: 8\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2014, accuracy : 56.6585%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2019, accuracy : 53.4610%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2000, accuracy : 60.5411%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.1990, accuracy : 64.4765%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1980, accuracy : 68.9740%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1964, accuracy : 72.9093%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1957, accuracy : 75.8257%\n",
      "Finished Training\n",
      "\n",
      "iteration: 9\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2037, accuracy : 35.9276%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1995, accuracy : 72.1363%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1968, accuracy : 89.1427%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 25%) loss : 0.2052, accuracy : 39.7400%\n",
      "disOnDataWin : 0m 3s (2 50%) loss : 0.2035, accuracy : 45.0457%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2021, accuracy : 50.7027%\n",
      "Finished Training\n",
      "\n",
      "iteration: 10\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.1970, accuracy : 86.3317%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2062, accuracy : 35.0668%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2044, accuracy : 43.0077%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2031, accuracy : 48.6297%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2024, accuracy : 50.4568%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.2010, accuracy : 56.4652%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.2002, accuracy : 58.5383%\n",
      "Finished Training\n",
      "\n",
      "iteration: 11\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1993, accuracy : 71.9782%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2039, accuracy : 45.4322%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2020, accuracy : 52.0028%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2009, accuracy : 56.5706%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2001, accuracy : 59.6275%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1989, accuracy : 63.7034%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1974, accuracy : 69.1848%\n",
      "Finished Training\n",
      "\n",
      "iteration: 12\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2015, accuracy : 54.8665%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2015, accuracy : 53.9002%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2000, accuracy : 60.8925%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1986, accuracy : 65.5657%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1978, accuracy : 67.2171%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1967, accuracy : 71.0822%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1954, accuracy : 76.2825%\n",
      "Finished Training\n",
      "\n",
      "iteration: 13\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2038, accuracy : 35.4181%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1996, accuracy : 71.7498%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1968, accuracy : 87.7372%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2049, accuracy : 39.9859%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2033, accuracy : 46.1349%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2018, accuracy : 52.8461%\n",
      "Finished Training\n",
      "\n",
      "iteration: 14\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1972, accuracy : 85.2776%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2060, accuracy : 36.1384%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2044, accuracy : 42.5509%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2028, accuracy : 49.0513%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2019, accuracy : 52.8110%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disOnDataWin : 0m 6s (5 71%) loss : 0.2010, accuracy : 55.4462%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1998, accuracy : 60.1546%\n",
      "Finished Training\n",
      "\n",
      "iteration: 15\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1994, accuracy : 71.3457%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2037, accuracy : 45.7484%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2022, accuracy : 51.0541%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2005, accuracy : 57.5193%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1999, accuracy : 59.9086%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1985, accuracy : 65.6360%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1979, accuracy : 67.0063%\n",
      "Finished Training\n",
      "\n",
      "iteration: 16\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2017, accuracy : 52.8988%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2011, accuracy : 55.1124%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.1996, accuracy : 62.1574%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1985, accuracy : 64.0900%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1975, accuracy : 69.0443%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1966, accuracy : 71.6093%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1952, accuracy : 76.3879%\n",
      "Finished Training\n",
      "\n",
      "iteration: 17\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2039, accuracy : 35.3303%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1996, accuracy : 70.3443%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1967, accuracy : 88.8616%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2045, accuracy : 41.7779%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2029, accuracy : 49.2973%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2020, accuracy : 52.4596%\n",
      "Finished Training\n",
      "\n",
      "iteration: 18\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1971, accuracy : 85.7871%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2057, accuracy : 37.0520%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2041, accuracy : 43.2888%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2030, accuracy : 48.1026%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2019, accuracy : 52.9515%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2006, accuracy : 56.8517%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1998, accuracy : 60.2600%\n",
      "Finished Training\n",
      "\n",
      "iteration: 19\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1994, accuracy : 71.2755%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2034, accuracy : 46.2228%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2018, accuracy : 51.8974%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2003, accuracy : 58.9951%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1995, accuracy : 61.8060%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1982, accuracy : 66.1982%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1973, accuracy : 68.0956%\n",
      "Finished Training\n",
      "\n",
      "iteration: 20\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2018, accuracy : 52.0028%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2010, accuracy : 55.9206%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.1996, accuracy : 61.4195%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1983, accuracy : 64.9684%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1972, accuracy : 69.9930%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1962, accuracy : 72.4526%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1951, accuracy : 76.5285%\n",
      "Finished Training\n",
      "\n",
      "iteration: 21\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2039, accuracy : 35.0843%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1996, accuracy : 70.5903%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1969, accuracy : 87.4912%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2047, accuracy : 41.4441%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2029, accuracy : 48.6297%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2017, accuracy : 52.0731%\n",
      "Finished Training\n",
      "\n",
      "iteration: 22\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1972, accuracy : 84.7154%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2057, accuracy : 37.7372%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2036, accuracy : 44.3781%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2029, accuracy : 48.6297%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2016, accuracy : 53.4434%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2003, accuracy : 58.2221%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1995, accuracy : 61.4898%\n",
      "Finished Training\n",
      "\n",
      "iteration: 23\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1995, accuracy : 69.5362%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2034, accuracy : 45.6430%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2016, accuracy : 53.1272%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2004, accuracy : 56.8166%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1994, accuracy : 61.6655%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1984, accuracy : 64.0548%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1972, accuracy : 68.9740%\n",
      "Finished Training\n",
      "\n",
      "iteration: 24\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2017, accuracy : 52.9164%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2010, accuracy : 54.9543%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.1997, accuracy : 59.9438%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.1983, accuracy : 65.7765%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1972, accuracy : 70.4498%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1960, accuracy : 72.6985%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1951, accuracy : 75.3338%\n",
      "Finished Training\n",
      "\n",
      "iteration: 25\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2040, accuracy : 35.1546%\n",
      "disOnGen : 0m 18s (2 50%) loss : 0.1997, accuracy : 70.5552%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1968, accuracy : 87.8777%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 25%) loss : 0.2046, accuracy : 41.2157%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2028, accuracy : 48.5242%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2015, accuracy : 53.1975%\n",
      "Finished Training\n",
      "\n",
      "iteration: 26\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1973, accuracy : 84.7505%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2053, accuracy : 38.0885%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2036, accuracy : 45.2214%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2026, accuracy : 48.9810%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2014, accuracy : 53.4434%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2005, accuracy : 57.2382%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1991, accuracy : 62.7899%\n",
      "Finished Training\n",
      "\n",
      "iteration: 27\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1994, accuracy : 71.6444%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2032, accuracy : 46.9431%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2014, accuracy : 53.8299%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2003, accuracy : 58.1518%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1991, accuracy : 62.7196%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1980, accuracy : 66.1630%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1970, accuracy : 68.8335%\n",
      "Finished Training\n",
      "\n",
      "iteration: 28\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2018, accuracy : 52.4069%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2010, accuracy : 54.2340%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.1994, accuracy : 60.7871%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.1982, accuracy : 65.2846%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1972, accuracy : 68.4118%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1960, accuracy : 73.0850%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1947, accuracy : 77.3366%\n",
      "Finished Training\n",
      "\n",
      "iteration: 29\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2039, accuracy : 34.6803%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1997, accuracy : 70.2038%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1970, accuracy : 85.6290%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2043, accuracy : 42.1117%\n",
      "disOnDataWin : 0m 3s (2 50%) loss : 0.2026, accuracy : 48.6297%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2015, accuracy : 52.0028%\n",
      "Finished Training\n",
      "\n",
      "iteration: 30\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1972, accuracy : 84.9965%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2051, accuracy : 38.6156%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2036, accuracy : 44.8700%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2022, accuracy : 50.5271%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2013, accuracy : 53.5840%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2001, accuracy : 58.2221%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1988, accuracy : 61.9115%\n",
      "Finished Training\n",
      "\n",
      "iteration: 31\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1995, accuracy : 70.1862%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2030, accuracy : 48.1377%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2011, accuracy : 54.2516%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2004, accuracy : 56.7463%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.1990, accuracy : 62.0871%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1978, accuracy : 65.6711%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1965, accuracy : 70.9417%\n",
      "Finished Training\n",
      "\n",
      "iteration: 32\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2017, accuracy : 52.9339%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1976, accuracy : 83.8018%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1950, accuracy : 94.3429%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2064, accuracy : 34.0478%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2049, accuracy : 40.8292%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2036, accuracy : 44.9403%\n",
      "Finished Training\n",
      "\n",
      "iteration: 33\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1952, accuracy : 92.3226%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2073, accuracy : 30.6044%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2054, accuracy : 36.5425%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2046, accuracy : 40.5481%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2033, accuracy : 47.0485%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2022, accuracy : 49.3324%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.2008, accuracy : 55.3760%\n",
      "Finished Training\n",
      "\n",
      "iteration: 34\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1975, accuracy : 82.8883%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2048, accuracy : 39.9859%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2033, accuracy : 46.2052%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2020, accuracy : 50.3162%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2008, accuracy : 54.6381%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.1999, accuracy : 59.2762%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1984, accuracy : 63.8089%\n",
      "Finished Training\n",
      "\n",
      "iteration: 35\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1999, accuracy : 68.0956%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2028, accuracy : 47.7688%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2008, accuracy : 54.8489%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1997, accuracy : 60.6465%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1984, accuracy : 64.7224%\n",
      "disOnDataWin : 0m 7s (5 71%) loss : 0.1974, accuracy : 67.2874%\n",
      "disOnDataWin : 0m 8s (6 85%) loss : 0.1962, accuracy : 72.0309%\n",
      "Finished Training\n",
      "\n",
      "iteration: 36\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 25%) loss : 0.2020, accuracy : 50.8784%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1977, accuracy : 82.4315%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1951, accuracy : 93.2888%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2058, accuracy : 36.1209%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2043, accuracy : 42.6564%\n",
      "disOnDataWin : 0m 3s (3 75%) loss : 0.2029, accuracy : 47.7512%\n",
      "Finished Training\n",
      "\n",
      "iteration: 37\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1953, accuracy : 92.4455%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2070, accuracy : 30.5868%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2053, accuracy : 38.9670%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2036, accuracy : 45.0808%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2026, accuracy : 48.6648%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2015, accuracy : 52.6704%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.2003, accuracy : 57.3085%\n",
      "Finished Training\n",
      "\n",
      "iteration: 38\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1978, accuracy : 81.3422%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2045, accuracy : 41.6725%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2026, accuracy : 49.1918%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2013, accuracy : 53.8299%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2005, accuracy : 56.7112%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1992, accuracy : 60.8573%\n",
      "disOnDataWin : 0m 8s (6 85%) loss : 0.1982, accuracy : 65.1441%\n",
      "Finished Training\n",
      "\n",
      "iteration: 39\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.2001, accuracy : 65.3900%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2020, accuracy : 50.8082%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2003, accuracy : 56.7463%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1993, accuracy : 61.0330%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1982, accuracy : 65.1089%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1970, accuracy : 69.0443%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1961, accuracy : 71.5039%\n",
      "Finished Training\n",
      "\n",
      "iteration: 40\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 25%) loss : 0.2023, accuracy : 48.2431%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1981, accuracy : 80.1827%\n",
      "disOnGen : 0m 26s (3 75%) loss : 0.1953, accuracy : 91.9185%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 25%) loss : 0.2054, accuracy : 37.7196%\n",
      "disOnDataWin : 0m 3s (2 50%) loss : 0.2038, accuracy : 44.3078%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2028, accuracy : 48.3837%\n",
      "Finished Training\n",
      "\n",
      "iteration: 41\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.1957, accuracy : 91.1630%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2065, accuracy : 33.5207%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2047, accuracy : 40.3373%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2036, accuracy : 44.8700%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2026, accuracy : 48.6297%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.2015, accuracy : 51.7569%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.2001, accuracy : 58.0815%\n",
      "Finished Training\n",
      "\n",
      "iteration: 42\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.1981, accuracy : 80.7098%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2041, accuracy : 43.3767%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2020, accuracy : 51.1595%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2013, accuracy : 53.7948%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2000, accuracy : 58.7843%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1991, accuracy : 61.2790%\n",
      "disOnDataWin : 0m 8s (6 85%) loss : 0.1979, accuracy : 65.2143%\n",
      "Finished Training\n",
      "\n",
      "iteration: 43\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.2003, accuracy : 64.7224%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2017, accuracy : 51.3879%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2002, accuracy : 56.4652%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1989, accuracy : 61.4898%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1974, accuracy : 67.5334%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1967, accuracy : 69.3254%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1957, accuracy : 73.7175%\n",
      "Finished Training\n",
      "\n",
      "iteration: 44\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 25%) loss : 0.2025, accuracy : 46.2228%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disOnGen : 0m 19s (2 50%) loss : 0.1982, accuracy : 79.3746%\n",
      "disOnGen : 0m 26s (3 75%) loss : 0.1956, accuracy : 91.8131%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2052, accuracy : 37.8426%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2034, accuracy : 44.7997%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2020, accuracy : 51.1947%\n",
      "Finished Training\n",
      "\n",
      "iteration: 45\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1957, accuracy : 90.2495%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2062, accuracy : 32.9585%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2045, accuracy : 40.1968%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2036, accuracy : 45.8890%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.2024, accuracy : 49.6135%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.2010, accuracy : 54.4273%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.2000, accuracy : 57.7301%\n",
      "Finished Training\n",
      "\n",
      "iteration: 46\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.1981, accuracy : 79.1110%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2038, accuracy : 43.7983%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2019, accuracy : 50.8082%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.2009, accuracy : 54.4624%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1997, accuracy : 58.9951%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1988, accuracy : 61.8763%\n",
      "disOnDataWin : 0m 7s (6 85%) loss : 0.1976, accuracy : 66.2684%\n",
      "Finished Training\n",
      "\n",
      "iteration: 47\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.2004, accuracy : 63.9494%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 2s (1 14%) loss : 0.2015, accuracy : 51.9677%\n",
      "disOnDataWin : 0m 3s (2 28%) loss : 0.2001, accuracy : 57.3436%\n",
      "disOnDataWin : 0m 4s (3 42%) loss : 0.1987, accuracy : 63.0358%\n",
      "disOnDataWin : 0m 5s (4 57%) loss : 0.1976, accuracy : 66.6901%\n",
      "disOnDataWin : 0m 6s (5 71%) loss : 0.1962, accuracy : 71.2579%\n",
      "disOnDataWin : 0m 8s (6 85%) loss : 0.1954, accuracy : 73.2256%\n",
      "Finished Training\n",
      "\n",
      "iteration: 48\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 25%) loss : 0.2027, accuracy : 45.0984%\n",
      "disOnGen : 0m 19s (2 50%) loss : 0.1983, accuracy : 79.2340%\n",
      "disOnGen : 0m 25s (3 75%) loss : 0.1955, accuracy : 92.5509%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 25%) loss : 0.2048, accuracy : 39.5994%\n",
      "disOnDataWin : 0m 2s (2 50%) loss : 0.2032, accuracy : 45.0457%\n",
      "disOnDataWin : 0m 4s (3 75%) loss : 0.2023, accuracy : 49.2973%\n",
      "Finished Training\n",
      "\n",
      "iteration: 49\n",
      "\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 13s (1 50%) loss : 0.1959, accuracy : 89.7927%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 14%) loss : 0.2058, accuracy : 36.1033%\n",
      "disOnDataWin : 0m 2s (2 28%) loss : 0.2045, accuracy : 41.6374%\n",
      "disOnDataWin : 0m 3s (3 42%) loss : 0.2029, accuracy : 47.2944%\n",
      "disOnDataWin : 0m 4s (4 57%) loss : 0.2020, accuracy : 50.3162%\n",
      "disOnDataWin : 0m 5s (5 71%) loss : 0.2007, accuracy : 55.5165%\n",
      "disOnDataWin : 0m 6s (6 85%) loss : 0.1994, accuracy : 58.6437%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train discriminator\n",
    "disOnGenWin = 1\n",
    "disOnDataWin = 1\n",
    "metaEpochs = 50\n",
    "paramsDisOnGen['epochs']=2\n",
    "paramsDisOnData['epochs']=2\n",
    "\n",
    "for i in range(metaEpochs):\n",
    "    print(\"\\niteration: %d\\n\" % i)\n",
    "    if disOnDataWin>0.75 or disOnDataWin>disOnGenWin:\n",
    "        paramsDisOnGen['epochs']=4\n",
    "        paramsDisOnData['epochs']=4\n",
    "\n",
    "    if disOnGenWin>0.75 or disOnGenWin>disOnDataWin:\n",
    "        paramsDisOnGen['epochs']=2\n",
    "        paramsDisOnData['epochs']=7\n",
    "        \n",
    "    disOnGenWin = max(myDis.trainOnGen(**paramsDisOnGen))\n",
    "    disOnDataWin = max(myDis.trainOnData(**paramsDisOnData))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnGen : 0m 12s (1 50%) loss : 0.0005, accuracy : 100.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 50%) loss : 0.7791, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 18s (1 50%) loss : 0.7450, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 50%) loss : 0.7085, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 18s (1 50%) loss : 0.6787, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 50%) loss : 0.6441, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 18s (1 50%) loss : 0.6162, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 50%) loss : 0.5820, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 18s (1 50%) loss : 0.5568, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "disOnDataWin : 0m 1s (1 50%) loss : 0.5245, accuracy : 0.0000%\n",
      "Finished Training\n",
      "Trying using Cuda ...\n",
      "OK\n",
      "Start training\n",
      "genWin : 0m 18s (1 50%) loss : 0.5021, accuracy : 0.0000%\n",
      "Finished Training\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "#Train Global\n",
    "paramsGen['epochs']=2\n",
    "paramsDisOnGen['epochs']=2\n",
    "paramsDisOnData['epochs']=2\n",
    "\n",
    "metaEpochs = 5\n",
    "\n",
    "genWin = 0\n",
    "disOnGenWin = 0\n",
    "disOnDataWin = 0\n",
    "\n",
    "for i in range(metaEpochs):\n",
    "    if disOnGenWin < 0.75:\n",
    "        disOnGenWin = max(myDis.trainOnGen(**paramsDisOnGen))\n",
    "    if disOnDataWin < 0.75:\n",
    "        disOnDataWin = max(myDis.trainOnData(**paramsDisOnData))\n",
    "    else:\n",
    "        genWin = max(myGenerator.trainOnDis(**paramsGen))\n",
    "\n",
    "print(\"finished\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n",
      "['C:maj', 'F:maj', 'C:maj', 'C:maj']\n",
      "['F:maj', 'F:maj', 'C:maj', 'C:maj']\n",
      "['G:maj', 'F:maj', 'C:maj', 'G:maj']\n",
      "['C:maj', 'C:maj', 'F:maj', 'G:maj']\n",
      "generated :\n",
      "['N', 'G:maj', 'D#:maj', 'A#:maj']\n",
      "['A:min', 'C:min', 'G:min', 'C#:min']\n",
      "['C:maj', 'A:maj', 'F#:min', 'F#:maj']\n",
      "['G#:maj', 'G#:maj', 'E:min', 'F:maj']\n",
      "['A#:min', 'A#:maj', 'D#:maj', 'D#:maj']\n",
      "['C:maj', 'N', 'A#:min', 'D:maj']\n",
      "['N', 'B:maj', 'E:maj', 'A#:min']\n",
      "['A:min', 'D#:maj', 'G:maj', 'N']\n",
      "['C#:maj', 'G#:maj', 'G#:maj', 'A#:min']\n",
      "['D#:maj', 'E:maj', 'N', 'E:min']\n",
      "['F:maj', 'B:min', 'G#:min', 'C#:min']\n",
      "['G#:min', 'F:maj', 'N', 'D#:min']\n"
     ]
    }
   ],
   "source": [
    "test_sequence = [\"C:maj\",\"F:maj\",\"C:maj\",\"C:maj\",\n",
    "                 \"F:maj\",\"F:maj\",\"C:maj\",\"C:maj\",\n",
    "                 \"G:maj\",\"F:maj\",\"C:maj\",\"G:maj\",\n",
    "                 \"C:maj\",\"C:maj\",\"F:maj\",\"G:maj\"]\n",
    "myGenerator.generateFromSequence(test_sequence, generation_lenght=64,\n",
    "                               alphabet = 'a0',sampling=True, \n",
    "                               using_cuda=True, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5141]], device='cuda:0', grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using_cuda = True\n",
    "# Cuda blabla\n",
    "if using_cuda:\n",
    "    print(\"Trying using Cuda ...\")\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"Woops, Cuda cannot be found :'( \")\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using Cpu\")\n",
    "\n",
    "lenSeq = len(test_sequence)\n",
    "\n",
    "\n",
    "# Getting chords dictionary\n",
    "rootname = \"inputs/jazz_xlab/\"\n",
    "filenames = os.listdir(rootname)\n",
    "dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "\n",
    "\n",
    "# Initialising objects\n",
    "test_sequence_tensor = torch.zeros(1, len(test_sequence), len(dictChord)).to(device)\n",
    "last_chords_output = torch.zeros(1, lenSeq, len(dictChord)).to(device)\n",
    "test_sequence_tensor.requires_grad = False\n",
    "last_chords_output.requires_grad = False\n",
    "for t in range(len(test_sequence)):\n",
    "    test_sequence_tensor[0, t, dictChord[test_sequence[t]]] = 1\n",
    "    \n",
    "#print(test_sequence_tensor.cpu())\n",
    "\n",
    "myDis(test_sequence_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
