{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "from utilities import chordUtil\n",
    "from utilities import dataImport\n",
    "from utilities import plotAndTimeUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities.dataImport import *\n",
    "from utilities.plotAndTimeUtil import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from utilities import chordsDistances\n",
    "from utilities.chordsDistances import getPaulMatrix\n",
    "from utilities import remapChordsToBase\n",
    "from utilities.remapChordsToBase import remapPaulToTristan\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYRNN(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_generator, num_layers_generator, dropout_generator):\n",
    "        super(MYRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size_generator, num_layers = num_layers_generator, batch_first = True, dropout = dropout_generator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_generator, input_size)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        output = self.softmax(output)\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        return output\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying using Cuda ...\n",
      "OK\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d0ef67b3609b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Using Cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "using_cuda = True\n",
    "\n",
    "if using_cuda:\n",
    "    print(\"Trying using Cuda ...\")\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        print(\"OK\")\n",
    "    else:\n",
    "        print(\"Woops, Cuda cannot be found :'( \")\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using Cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "from utilities import chordUtil\n",
    "from utilities import dataImport\n",
    "from utilities import plotAndTimeUtil\n",
    "from utilities.chordUtil import *\n",
    "from utilities.dataImport import *\n",
    "from utilities.plotAndTimeUtil import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from utilities import chordsDistances\n",
    "from utilities.chordsDistances import getPaulMatrix\n",
    "from utilities import remapChordsToBase\n",
    "from utilities.remapChordsToBase import remapPaulToTristan\n",
    "from random import choices\n",
    "\n",
    "\n",
    "class MYRNN(nn.Module):\n",
    "\n",
    "    # Defining Pytorch things to make the Network\n",
    "\n",
    "    def __init__(self, input_size, hidden_size_generator, num_layers_generator, dropout_generator):\n",
    "        super(MYRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size_generator, num_layers = num_layers_generator, batch_first = True, dropout = dropout_generator)\n",
    "        self.last_fully_connected = nn.Linear(hidden_size_generator, input_size)\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        \n",
    "    # Defining some cool functions to make work easier and trop styl√©\n",
    "\n",
    "    # About the trainingData :\n",
    "    # 0 is training number\n",
    "    # 1 is optimizer used\n",
    "    # 2 is loss used\n",
    "    # 3 is loss values on train set\n",
    "    # 4 is loss values on test set\n",
    "    # 5 is accuracy on train set\n",
    "    # 6 is accuracy on test set\n",
    "    # 7 is time\n",
    "    # 8 is learning rate\n",
    "    # 9 is use_Paul_distance\n",
    "\n",
    "        \n",
    "        # Usefull for monitoring\n",
    "        self.trainingData = [[0],[\"None\"],[\"None\"],[[0]],[[0]],[[0]],[[0]],[0],[0],[\"No\"]]\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        output, hidden = self.rnn(input_batch)\n",
    "        output = output[:,-1,:]\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        output = self.last_fully_connected(output)\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        # not sure about the dim = 1 in softmax\n",
    "        output = self.softmax(output)\n",
    "        print(\"outputsize : \")\n",
    "        print(output.size())\n",
    "        return output\n",
    "\n",
    "    def trainOnDis(self, model_type, print_every, optimizer, lossFunction, data_set, disNet, alphabet='a0', sequence_lenght_in=16, sequence_lenght_out=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size_generator=128, num_layers_generator=2, dropout_generator=0.1, learning_rate_generator=1e-4, epochs=10, sampling=True):\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        all_losses, similarity = self.doEpochs(model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, data_set, disNet, sampling, training=True)\n",
    "\n",
    "        self.trainingData[0].append(self.trainingData[0][-1]+1)\n",
    "        self.trainingData[1].append(optimizer)\n",
    "        self.trainingData[2].append(lossFunction)\n",
    "        self.trainingData[3].append(all_losses)\n",
    "        self.trainingData[4].append(test_losses)\n",
    "        self.trainingData[5].append(accuracy_train)\n",
    "        self.trainingData[6].append(accuracy_test)\n",
    "        self.trainingData[7].append(plotAndTimeUtil.timeSince(start))\n",
    "        self.trainingData[8].append(learning_rate)\n",
    "        self.trainingData[9].append(use_Paul_distance)\n",
    "        \n",
    "        print(\"Finished Training\")\n",
    "\n",
    "\n",
    "        return\n",
    "\n",
    "    def doEpochs(self, model_type, epochs, print_every, optimizer, lossFunction, learning_rate_generator, batch_size, shuffle, num_workers, alphabet, sequence_lenght_in, sequence_lenght_out, device, start, data_set, disNet, sampling, training=True):\n",
    "\n",
    "        # Init Training results and monitoring data\n",
    "        all_losses = []\n",
    "        total_loss = 0 # Reset every plot_every iters\n",
    "        genWin = 0\n",
    "        totalSize = 0\n",
    "        genWinTest = []\n",
    "\n",
    "        # Creating Dataset\n",
    "            # dataset has been passed in the parameters\n",
    "        \n",
    "        # Create generators\n",
    "        params = {'batch_size': batch_size,\n",
    "                  'shuffle': shuffle,\n",
    "                  'num_workers': num_workers}\n",
    "            \n",
    "        if training:\n",
    "            training_generator = data.DataLoader(dataset, **params)\n",
    "\n",
    "        #TODO Put more optimisers\n",
    "        if optimizer == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is unknown to me\")\n",
    "\n",
    "        #TODO Put more losses\n",
    "        if lossFunction == \"CrossEntropy\":\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        elif lossFunction == \"MSE\":\n",
    "            criterion = nn.MSELoss()\n",
    "        else:\n",
    "            raise ValueError(\"This loss function is unknown to me\")\n",
    "\n",
    "        if training:\n",
    "            print(\"Start training\")\n",
    "            \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            if training:\n",
    "                self.train(mode=True)\n",
    "                for local_batch, local_labels in training_generator:\n",
    "                    output, loss, genWinOne = self.oneBatchTrain(local_batch, local_labels, optimizer, criterion, device, sampling)\n",
    "                    total_loss += loss\n",
    "                    genWin += genWinOne\n",
    "                    totalSize += len(local_batch)\n",
    "\n",
    "                if epoch % print_every == 0 and epoch != 0:\n",
    "                    genWin = genWin/totalSize\n",
    "                    total_loss = total_loss/totalSize\n",
    "                    genWinTest.append(genWin)\n",
    "                    all_losses.append(total_loss)\n",
    "                    print('genWin : %s (%d %d%%) loss : %.4f, accuracy : %.4f%%' % (plotAndTimeUtil.timeSince(start), epoch, epoch / epochs * 100, total_loss, genWin*100))\n",
    "                    genWin = 0\n",
    "                    total_loss = 0\n",
    "                    \n",
    "            # Testing\n",
    "            #self.train(mode=False)\n",
    "\n",
    "        return all_losses, genWinTest\n",
    "\n",
    "\n",
    "\n",
    "# define Train on one batch function\n",
    "\n",
    "    def oneBatchTrain(self, local_batch, local_labels, optimizer, criterion, device, sequence_lenght_in, sequence_lenght_out, sampling):\n",
    "       \n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        correct_guess = 0\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "        # if tensor of shape 1 in loss function (ex : CrossEntropy)\n",
    "        #local_labels_argmax = torch.tensor([torch.argmax(local_label) for local_label in local_labels]).to(device)\n",
    "        \n",
    "# THIS PART IS NOT FINISHED AT ALLLLLL\n",
    "\n",
    "        local_batch = local_batch.to(device)\n",
    "        #local_labels = local_labels.to(device)\n",
    "        for i in range(sequence_lenght_out):\n",
    "            output = self.forward(local_batch)\n",
    "            local_batch[:,0:sequence_lenght_in-1,:] = local_batch[:,1:sequence_lenght_in,:]\n",
    "            \n",
    "            if sampling:\n",
    "                choice = choices(range(len(listChord)),softmax(output[0]))[0]\n",
    "                local_batch[:,sequence_lenght_in, choice] = 1\n",
    "            else:\n",
    "                generated_sequence[sequence_lenght_in] = listChord[torch.argmax(output_probability).item()]\n",
    "                local_batch[:, sequence_lenght_in, torch.argmax(output[:,:]).item()] = 1                \n",
    "                \n",
    "        \n",
    "        for i in range(len(local_batch)):\n",
    "            if torch.argmax(output[i]) == torch.argmax(local_labels[i]):\n",
    "                correct_guess += 1\n",
    "            else:\n",
    "                wrong_guess += 1\n",
    "                \n",
    "\n",
    "        loss = loss_mult_coeff * criterion(output, local_labels_argmax)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return output, loss.item(), correct_guess\n",
    "\n",
    "\n",
    "\n",
    "    def plotLastTraining(self):\n",
    "        plotAndTimeUtil.PlotResults(self.trainingData[3][-1], self.trainingData[4][-1], self.trainingData[5][-1], self.trainingData[6][-1])\n",
    "\n",
    "    def plotAllTraining(self):\n",
    "        plotAndTimeUtil.PlotAllResults(self.trainingData)\n",
    "\n",
    "    def toString(self, model_type, print_every, plot_every, optimizer, lossFunction, alphabet='a0', sequence_lenght=16, using_cuda=True, batch_size=128, shuffle=True, num_workers=6, hidden_size=128, num_layers=2, dropout=0.1, learning_rate=1e-4, epochs=10, use_Paul_distance=False):\n",
    "        #dropoutstr = str(dropout).replace('.',',')\n",
    "        model_string = \"models/\"+model_type+str(num_layers)+\"layers\"+str(hidden_size)+\"blocks\"+alphabet+\"alphabet\"+str(sequence_lenght)+\"lenSeq.pt\"\n",
    "        return model_string\n",
    "    \n",
    "    def generateFromSequence(self, test_sequence, generation_lenght, alphabet, sampling=False, using_cuda=True, silent=True):\n",
    "        lenSeq = len(test_sequence)\n",
    "        \n",
    "        # Cuda blabla\n",
    "        if using_cuda:\n",
    "            print(\"Trying using Cuda ...\")\n",
    "            use_cuda = torch.cuda.is_available()\n",
    "            if use_cuda:\n",
    "                print(\"OK\")\n",
    "            else:\n",
    "                print(\"Woops, Cuda cannot be found :'( \")\n",
    "            device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using Cpu\")\n",
    "        self.to(device)    \n",
    "        \n",
    "        # Getting chords dictionary\n",
    "        rootname = \"inputs/jazz_xlab/\"\n",
    "        filenames = os.listdir(rootname)\n",
    "        dictChord, listChord = chordUtil.getDictChord(eval(alphabet))\n",
    "        \n",
    "        \n",
    "        # Initialising objects\n",
    "        test_sequence_tensor = torch.zeros(1, len(test_sequence), len(dictChord)).to(device)\n",
    "        last_chords_output = torch.zeros(1, lenSeq, len(dictChord)).to(device)\n",
    "        test_sequence_tensor.requires_grad = False\n",
    "        last_chords_output.requires_grad = False\n",
    "        for t in range(len(test_sequence)):\n",
    "            test_sequence_tensor[0, t, dictChord[test_sequence[t]]] = 1\n",
    "            if t != len(test_sequence)-1 :\n",
    "                last_chords_output[0, t-1, dictChord[test_sequence[t]]] = 1\n",
    "\n",
    "                \n",
    "        generated_sequence = [0 for i in range(generation_lenght)]\n",
    "        generated_sequence[0:lenSeq] = test_sequence\n",
    "\n",
    "        self.train(mode=False)\n",
    "        softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        for t in range(generation_lenght-lenSeq):\n",
    "            if t == 0:\n",
    "                output_probability = self(test_sequence_tensor)\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else: \n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output_probability).item()]\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                last_chords_output.to(device)        \n",
    "                output_probability = self(last_chords_output)\n",
    "                last_chords_output[0,0:lenSeq-1] = last_chords_output[0,1:lenSeq]\n",
    "\n",
    "                if sampling:\n",
    "                    choice = choices(range(len(listChord)),softmax(output_probability[0]))[0]\n",
    "                    generated_sequence[t+lenSeq] = listChord[choice]\n",
    "                    last_chords_output[0, lenSeq-1, choice] = 1\n",
    "\n",
    "                else:\n",
    "                    last_chords_output[0, lenSeq-1, torch.argmax(output_probability).item()] = 1\n",
    "                    generated_sequence[t+lenSeq] = listChord[torch.argmax(output).item()]\n",
    "\n",
    "\n",
    "        for i in range(generation_lenght):\n",
    "            if i%4 == 0:\n",
    "                print(generated_sequence[i:i+4])\n",
    "            if i == lenSeq-1 :\n",
    "                print(\"generated :\")\n",
    "                \n",
    "        if silent:\n",
    "            return\n",
    "        else:\n",
    "            return generated_sequence\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
